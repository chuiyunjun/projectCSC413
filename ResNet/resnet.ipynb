{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models\nimport string\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n!pip install adabelief-pytorch==0.2.0\nfrom adabelief_pytorch import AdaBelief\nfrom torch.optim import *\nimport math\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting adabelief-pytorch==0.2.0\n  Downloading adabelief_pytorch-0.2.0-py3-none-any.whl (5.7 kB)\nRequirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from adabelief-pytorch==0.2.0) (1.7.0)\nRequirement already satisfied: colorama>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from adabelief-pytorch==0.2.0) (0.4.3)\nRequirement already satisfied: tabulate>=0.7 in /opt/conda/lib/python3.7/site-packages (from adabelief-pytorch==0.2.0) (0.8.7)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (3.7.4.1)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (1.18.5)\nInstalling collected packages: adabelief-pytorch\nSuccessfully installed adabelief-pytorch-0.2.0\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# import os\n# os.listdir('../input/emnist')\n# os.listdir('data/EMNIST/processed/')","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5), (0.5))])\n\ntrain_set = torchvision.datasets.EMNIST(root='./data', train=True, download=True, transform=transform, split='bymerge')\ntest_set = torchvision.datasets.EMNIST(root='./data', train=False, download=False, transform=transform, split='bymerge')","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda:0\nDownloading and extracting zip archive\nDownloading http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/emnist.zip\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86205118bd404619ae6e9ca1c7a0d133"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/EMNIST/raw/emnist.zip to ./data/EMNIST/raw\nProcessing byclass\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n","output_type":"stream"},{"name":"stdout","text":"Processing bymerge\nProcessing balanced\nProcessing letters\nProcessing digits\nProcessing mnist\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"class ResNet(nn.Module):\n    \n    def __init__(self, res, linear_dim=128, output_dim=47):\n        \n        super().__init__()\n        self.resnet = res\n        self.linear = nn.Linear(linear_dim, output_dim)\n    \n    def forward(self, x):\n        \n        x = self.resnet(x)\n        x = self.linear(x.squeeze())\n        \n        return x.squeeze(1)","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, valid_dl):\n\n  # we don't want to add evaluation related computation to computation graph\n  with torch.no_grad():\n    criterion = nn.CrossEntropyLoss()\n    valid_loss_lst = torch.tensor([]).to('cuda')\n    y_true_lst = torch.tensor([]).to('cuda')\n    y_pred_lst = torch.tensor([]).to('cuda')\n    \n    for X_valid, y_valid in valid_dl:\n      X_valid = X_valid.to('cuda')\n      y_valid = y_valid.to('cuda')\n      pred = model(X_valid)\n      valid_loss_lst = torch.cat((valid_loss_lst, criterion(pred, y_valid).view(-1)))\n      predicted = pred.argmax(1).squeeze()\n      y_pred_lst = torch.cat((y_pred_lst, predicted))\n      y_true_lst = torch.cat((y_true_lst, y_valid))\n\n  return valid_loss_lst.mean().item(), f1_score(y_pred=y_pred_lst.to('cpu'), y_true=y_true_lst.to('cpu'), average='micro')\n\n\ndef train_res(**params):\n\n  # define model and move it to gpu\n  model = params['model']\n  model.to('cuda')\n\n  # define optimizer\n  opt = params['optimizer'][0](model.parameters(), **params['optimizer'][1])\n\n  # define data loader\n  \n  train_dl = DataLoader(params['train_data'], batch_size=params['batch_size'])\n  valid_dl = DataLoader(params['test_data'], batch_size=params['batch_size'])\n\n  # define loss\n  criterion = torch.nn.CrossEntropyLoss()\n\n  # start training\n  valid_loss_lst = []\n  train_loss_lst = []\n  train_f1_lst = []\n  valid_f1_lst = []\n\n  for e in tqdm(range(params['epochs'] + 1)):\n    train_losses = torch.tensor([]).to('cuda')\n    correct = 0\n    y_true_lst = torch.tensor([]).to('cuda')\n    y_pred_lst = torch.tensor([]).to('cuda')\n    \n    for X_train, y_train in train_dl:\n\n      X_train = X_train.to('cuda')\n      y_train = y_train.to('cuda')\n        \n      pred = model(X_train)\n      loss = criterion(pred, y_train)\n\n      # perform backward propagation\n      loss.backward()\n\n      # update model parameters and clear up gradient after update\n      opt.step()\n      opt.zero_grad()\n      train_losses = torch.cat((train_losses, loss.view(-1)))\n\n      with torch.no_grad():\n        predicted = pred.argmax(1).squeeze()\n        y_pred_lst = torch.cat((y_pred_lst, predicted))\n        y_true_lst = torch.cat((y_true_lst, y_train))\n      \n        \n    valid_loss, valid_f1 = evaluate(model, valid_dl)\n    train_loss = train_losses.mean().item()\n    train_f1 = f1_score(y_pred=y_pred_lst.to('cpu'), y_true=y_true_lst.to('cpu'), average='micro')\n    train_loss_lst.append(train_loss)\n    valid_loss_lst.append(valid_loss)\n    train_f1_lst.append(train_f1)\n    valid_f1_lst.append(valid_f1)\n    \n\n    print(f'epoch: {e}, training loss: {train_loss}, training f1: {train_f1}, validation loss: {valid_loss}, validation f1: {valid_f1}')\n\n  return model, train_loss_lst, valid_loss_lst, train_f1_lst, valid_f1_lst\n\n","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Padam(Optimizer):\n    \"\"\"Implements Partially adaptive momentum estimation (Padam) algorithm.\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-1)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        partial (float, optional): partially adaptive parameter\n    \"\"\"\n\n    def __init__(self, params, lr=1e-1, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=True, partial = 1/4):\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad, partial = partial)\n        super(Padam, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsgrad = group['amsgrad']\n                partial = group['partial']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsgrad:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                if group['weight_decay'] != 0:\n                    grad = grad.add(group['weight_decay'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom**(partial*2))\n                \n        return loss\n\ndef get_resnet(state_dict=None):\n\n  resnet18 = models.resnet18()\n  emnist_cov2d = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  new_resnet18 = nn.Sequential(*[emnist_cov2d, resnet18.bn1, resnet18.relu, resnet18.maxpool] + list(resnet18.layer1) + [resnet18.avgpool])\n  out_resnet = ResNet(new_resnet18, 64)\n\n  if state_dict is not None:\n    out_resnet.load_state_dict(state_dict)\n  \n  return out_resnet","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"my_resnet_adam = get_resnet()\ndefault_params = my_resnet_adam.state_dict()\nmy_resnet_adam_belief = get_resnet(default_params)\nmy_resnet_padam = get_resnet(default_params)\nmy_resnet_SGD = get_resnet(default_params)","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Momentum","metadata":{}},{"cell_type":"code","source":"params = {\n    'optimizer': (torch.optim.SGD, {'lr': 0.0005, 'momentum': 0.9}),\n    'model': my_resnet_SGD,\n    'epochs': 30,\n    'batch_size': 64,\n    'train_data': train_set,\n    'test_data': test_set\n    \n}\nmodel_SGD, train_loss_lst_SGD, valid_loss_lst_SGD, train_f1_lst_SGD, valid_f1_lst_SGD = train_res(**params)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"  3%|▎         | 1/31 [02:50<1:25:11, 170.39s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 0, training loss: 1.2380510568618774, training f1: 0.6810018741080793, validation loss: 0.6047092080116272, validation f1: 0.8188492387575974\n","output_type":"stream"},{"name":"stderr","text":"  6%|▋         | 2/31 [05:38<1:22:04, 169.81s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 1, training loss: 0.49346446990966797, training f1: 0.8460838591725268, validation loss: 0.43002229928970337, validation f1: 0.8608873567566173\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 3/31 [08:26<1:19:00, 169.29s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 2, training loss: 0.39502349495887756, training f1: 0.8694371371423004, validation loss: 0.3743441104888916, validation f1: 0.8742810966017038\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 4/31 [11:15<1:16:07, 169.16s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 3, training loss: 0.3558635711669922, training f1: 0.8789466595599571, validation loss: 0.347007691860199, validation f1: 0.8810467405414234\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 5/31 [14:04<1:13:12, 168.93s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 4, training loss: 0.33417364954948425, training f1: 0.8842695276903767, validation loss: 0.3309228718280792, validation f1: 0.8848465049904146\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 6/31 [16:51<1:10:13, 168.55s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 5, training loss: 0.31997472047805786, training f1: 0.8880292062837067, validation loss: 0.31979015469551086, validation f1: 0.887812384481143\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 7/31 [19:41<1:07:35, 168.98s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 6, training loss: 0.3096529543399811, training f1: 0.8908231747505487, validation loss: 0.3116424083709717, validation f1: 0.8900475400393731\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 8/31 [22:30<1:04:46, 168.96s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 7, training loss: 0.3016294836997986, training f1: 0.8930612151327063, validation loss: 0.3052513301372528, validation f1: 0.8917582937166338\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 9/31 [25:21<1:02:06, 169.40s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 8, training loss: 0.2950819730758667, training f1: 0.89471467134334, validation loss: 0.30022376775741577, validation f1: 0.8928156942307196\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 10/31 [28:09<59:10, 169.09s/it] ","output_type":"stream"},{"name":"stdout","text":"epoch: 9, training loss: 0.28958839178085327, training f1: 0.8962578016196421, validation loss: 0.29597944021224976, validation f1: 0.8937011597018646\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 11/31 [30:57<56:13, 168.68s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 10, training loss: 0.28486010432243347, training f1: 0.8975301318753116, validation loss: 0.29220524430274963, validation f1: 0.8946811894466271\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▊      | 12/31 [33:44<53:18, 168.35s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 11, training loss: 0.28071898221969604, training f1: 0.8986348240229707, validation loss: 0.2891400158405304, validation f1: 0.8958159607300362\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 13/31 [36:32<50:26, 168.14s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 12, training loss: 0.27704551815986633, training f1: 0.8994701489543394, validation loss: 0.28633949160575867, validation f1: 0.8964865073975052\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 14/31 [39:23<47:50, 168.86s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 13, training loss: 0.27376800775527954, training f1: 0.9003298315595215, validation loss: 0.28388190269470215, validation f1: 0.8970281027827687\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 15/31 [42:10<44:56, 168.54s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 14, training loss: 0.27079927921295166, training f1: 0.9012095734254914, validation loss: 0.28170815110206604, validation f1: 0.8976986494502377\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 16/31 [44:58<42:06, 168.41s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 15, training loss: 0.2680900990962982, training f1: 0.9019374380312122, validation loss: 0.2798425257205963, validation f1: 0.8981198903054426\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▍    | 17/31 [47:45<39:08, 167.75s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 16, training loss: 0.2656031548976898, training f1: 0.9026036920502285, validation loss: 0.27802735567092896, validation f1: 0.8986958726992942\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 18/31 [50:33<36:22, 167.92s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 17, training loss: 0.26329711079597473, training f1: 0.903248454004115, validation loss: 0.2764890491962433, validation f1: 0.899108516802352\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████▏   | 19/31 [53:22<33:38, 168.19s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 18, training loss: 0.26116612553596497, training f1: 0.9038975143710275, validation loss: 0.2751167416572571, validation f1: 0.8994437901360866\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▍   | 20/31 [56:12<30:55, 168.72s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 19, training loss: 0.2591720223426819, training f1: 0.9043531461517741, validation loss: 0.2737584114074707, validation f1: 0.8996501121876155\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 21/31 [59:02<28:12, 169.27s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 20, training loss: 0.2572941184043884, training f1: 0.9049062086277746, validation loss: 0.27252399921417236, validation f1: 0.9000455627863793\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████   | 22/31 [1:01:52<25:24, 169.39s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 21, training loss: 0.25554296374320984, training f1: 0.9054076901474643, validation loss: 0.2713959515094757, validation f1: 0.90021749782932\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 23/31 [1:04:41<22:34, 169.34s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 22, training loss: 0.2538736164569855, training f1: 0.9058117409718999, validation loss: 0.27035588026046753, validation f1: 0.9003464491115256\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 24/31 [1:07:29<19:42, 168.99s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 23, training loss: 0.25229471921920776, training f1: 0.9063031928611957, validation loss: 0.2694977819919586, validation f1: 0.9004582068894371\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 25/31 [1:10:20<16:56, 169.44s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 24, training loss: 0.2508091628551483, training f1: 0.906672856381424, validation loss: 0.26863452792167664, validation f1: 0.9006731256931132\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 26/31 [1:13:10<14:08, 169.66s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 25, training loss: 0.24938644468784332, training f1: 0.907105563292699, validation loss: 0.26781806349754333, validation f1: 0.9007934802231717\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 27/31 [1:15:59<11:17, 169.36s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 26, training loss: 0.24802924692630768, training f1: 0.9075425686169999, validation loss: 0.2671981453895569, validation f1: 0.9007676899667306\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 28/31 [1:18:48<08:27, 169.21s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 27, training loss: 0.24673320353031158, training f1: 0.9078305622897359, validation loss: 0.26655253767967224, validation f1: 0.9008192704796129\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▎| 29/31 [1:21:37<05:38, 169.28s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 28, training loss: 0.24547895789146423, training f1: 0.9081572416797051, validation loss: 0.2659539580345154, validation f1: 0.9007934802231717\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 30/31 [1:24:26<02:49, 169.19s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 29, training loss: 0.24426354467868805, training f1: 0.908499681917436, validation loss: 0.26542407274246216, validation f1: 0.9009052380010832\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [1:27:14<00:00, 168.85s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 30, training loss: 0.2431037425994873, training f1: 0.9088191972856955, validation loss: 0.26492905616760254, validation f1: 0.9010599795397299\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nsgd_df = pd.DataFrame(data={'train_loss': train_loss_lst_SGD, \n                            'valid_loss': valid_loss_lst_SGD, \n                            'train_f1': train_f1_lst_SGD, \n                            'valid_f1': valid_f1_lst_SGD, \n                            'category': ['sgd']*len(train_loss_lst_SGD)})\n\nsgd_df.to_parquet('./sgd_results.parquet')","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## ADAM","metadata":{}},{"cell_type":"code","source":"params = {\n    'optimizer': (torch.optim.Adam, {'lr': 0.0005}),\n    'model': my_resnet_adam,\n    'epochs': 30,\n    'batch_size': 64,\n    'train_data': train_set,\n    'test_data': test_set\n    \n}\nmodel, train_loss_lst, valid_loss_lst, train_f1_lst, valid_f1_lst = train_res(**params)","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  3%|▎         | 1/31 [02:58<1:29:04, 178.16s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 0, training loss: 0.4217078983783722, training f1: 0.8625897651920246, validation loss: 0.30546167492866516, validation f1: 0.8898068309792561\n","output_type":"stream"},{"name":"stderr","text":"  6%|▋         | 2/31 [05:56<1:26:10, 178.29s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 1, training loss: 0.29105445742607117, training f1: 0.89365296332594, validation loss: 0.2811935544013977, validation f1: 0.8967616034662105\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 3/31 [08:53<1:23:01, 177.92s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 2, training loss: 0.27204471826553345, training f1: 0.8992523626943599, validation loss: 0.2716762125492096, validation f1: 0.899555547913998\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 4/31 [11:50<1:19:56, 177.64s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 3, training loss: 0.26072195172309875, training f1: 0.9027613005278451, validation loss: 0.2663037180900574, validation f1: 0.9009482217618185\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 5/31 [14:47<1:16:52, 177.42s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 4, training loss: 0.2527416944503784, training f1: 0.9049577895840856, validation loss: 0.26291558146476746, validation f1: 0.9015585911642582\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 6/31 [17:44<1:13:53, 177.35s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 5, training loss: 0.24651475250720978, training f1: 0.9068147040112791, validation loss: 0.2603242099285126, validation f1: 0.901971235267316\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 7/31 [20:41<1:10:53, 177.24s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 6, training loss: 0.24138644337654114, training f1: 0.9083420734398193, validation loss: 0.2584672272205353, validation f1: 0.9025042339004324\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 8/31 [23:39<1:07:58, 177.31s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 7, training loss: 0.23685644567012787, training f1: 0.9097619825427119, validation loss: 0.25673234462738037, validation f1: 0.9032607480893718\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 9/31 [26:36<1:04:58, 177.22s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 8, training loss: 0.23281845450401306, training f1: 0.9109440461248374, validation loss: 0.2564118802547455, validation f1: 0.9034584733887537\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 10/31 [29:32<1:01:57, 177.03s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 9, training loss: 0.22918762266635895, training f1: 0.9119785308597399, validation loss: 0.2561969757080078, validation f1: 0.9033982961237245\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 11/31 [32:30<59:02, 177.13s/it]  ","output_type":"stream"},{"name":"stdout","text":"epoch: 10, training loss: 0.22586195170879364, training f1: 0.9130287764424041, validation loss: 0.2563183009624481, validation f1: 0.9037163759531649\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▊      | 12/31 [35:27<56:03, 177.03s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 11, training loss: 0.22276847064495087, training f1: 0.9141048125032238, validation loss: 0.2563476860523224, validation f1: 0.9039742785175761\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 13/31 [38:24<53:08, 177.13s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 12, training loss: 0.2196711301803589, training f1: 0.9149473014563023, validation loss: 0.25724178552627563, validation f1: 0.9037077792010179\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 14/31 [41:21<50:12, 177.22s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 13, training loss: 0.21676623821258545, training f1: 0.9158055512571426, validation loss: 0.25806891918182373, validation f1: 0.9036819889445767\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 15/31 [44:19<47:17, 177.32s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 14, training loss: 0.21405504643917084, training f1: 0.9166609354492988, validation loss: 0.25900548696517944, validation f1: 0.9036390051838415\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 16/31 [47:18<44:26, 177.77s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 15, training loss: 0.21136878430843353, training f1: 0.9175636021847401, validation loss: 0.26032528281211853, validation f1: 0.9035014571494889\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▍    | 17/31 [50:16<41:31, 177.97s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 16, training loss: 0.208754763007164, training f1: 0.9184419112463679, validation loss: 0.26199862360954285, validation f1: 0.9031146033028722\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 18/31 [53:15<38:37, 178.30s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 17, training loss: 0.2062470018863678, training f1: 0.9191998647432701, validation loss: 0.26326075196266174, validation f1: 0.9031403935593133\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████▏   | 19/31 [56:15<35:44, 178.71s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 18, training loss: 0.2037435621023178, training f1: 0.9199635494575403, validation loss: 0.26498323678970337, validation f1: 0.9026761689433732\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▍   | 20/31 [59:13<32:45, 178.64s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 19, training loss: 0.20134477317333221, training f1: 0.9207429950195721, validation loss: 0.26664745807647705, validation f1: 0.9024956371482854\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 21/31 [1:02:13<29:48, 178.84s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 20, training loss: 0.19888976216316223, training f1: 0.9215525294727853, validation loss: 0.2685604691505432, validation f1: 0.9022377345838744\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████   | 22/31 [1:05:12<26:51, 179.07s/it]","output_type":"stream"},{"name":"stdout","text":"epoch: 21, training loss: 0.19666428864002228, training f1: 0.9222101866657497, validation loss: 0.27028027176856995, validation f1: 0.9019798320194631\n","output_type":"stream"}]},{"cell_type":"code","source":"adam_df = belif_df = pd.DataFrame(data={'train_loss': train_loss_lst, \n                            'valid_loss': valid_loss_lst, \n                            'train_f1': train_f1_lst, \n                            'valid_f1': valid_f1_lst, \n                            'category': ['adam']*len(train_loss_lst_SGD)})\n\nadam_df.to_parquet('./adam_results.parquet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBelif","metadata":{}},{"cell_type":"code","source":"params = {\n    'optimizer': (AdaBelief, {'lr': 0.0005}),\n    'model': my_resnet_adam_belief,\n    'epochs': 30,\n    'batch_size': 64,\n    'train_data': train_set,\n    'test_data': test_set\n    \n}\n\nmodel_belief, train_loss_lst_belief, valid_loss_lst_belief, train_f1_lst_belief, valid_f1_lst_belief = train_res(**params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"belif_df = pd.DataFrame(data={'train_loss': train_loss_lst_belief, \n                            'valid_loss': valid_loss_lst_belief, \n                            'train_f1': train_f1_lst_belief, \n                            'valid_f1': valid_f1_lst_belief, \n                            'category': ['belief']*len(train_loss_lst_SGD)})\n\nbelif_df.to_parquet('./belif_results.parquet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Padam","metadata":{}},{"cell_type":"code","source":"params = {\n    'optimizer': (Padam, {'lr': 0.0005}),\n    'model': my_resnet_padam,\n    'epochs': 30,\n    'batch_size': 64,\n    'train_data': train_set,\n    'test_data': test_set\n    \n}\n\nmodel_padam, train_loss_lst_padam, valid_loss_lst_padam, train_f1_lst_padam, valid_f1_lst_padam = train_res(**params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padam_df = pd.DataFrame(data={'train_loss': train_loss_lst_padam, \n                            'valid_loss': valid_loss_lst_padam, \n                            'train_f1': train_f1_lst_padam, \n                            'valid_f1': valid_f1_lst_padam, \n                            'category': ['padam']*len(train_loss_lst_SGD)})\n\npadam_df.to_parquet('./padam_results.parquet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df = pd.concat([belif_df, adam_df, padam_df, sgd_df])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}